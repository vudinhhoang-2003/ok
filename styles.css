import java.io.IOException;
import java.util.Iterator;
import java.util.StringTokenizer;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;

public class ProcessUnits {

    // Mapper class
    public static class E_Mapper extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {
        // Map function
        public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {
            String line = value.toString();
            String lasttoken = null;
            StringTokenizer s = new StringTokenizer(line, "\t");
            String year = s.nextToken();

            while (s.hasMoreTokens()) {
                lasttoken = s.nextToken();
            }

            int avgprice = Integer.parseInt(lasttoken);
            output.collect(new Text(year), new IntWritable(avgprice));
        }
    }

    // Reducer class
    public static class E_Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {
        // Reduce function
        public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {
            int maxavg = 30;
            int val = Integer.MIN_VALUE;

            while (values.hasNext()) {
                if ((val = values.next().get()) > maxavg) {
                    output.collect(key, new IntWritable(val));
                }
            }
        }
    }

    // Main function
    public static void main(String args[]) throws Exception {
        JobConf conf = new JobConf(ProcessUnits.class);
        conf.setJobName("max_electricityunits");

        conf.setOutputKeyClass(Text.class);
        conf.setOutputValueClass(IntWritable.class);

        conf.setMapperClass(E_Mapper.class);
        conf.setCombinerClass(E_Reduce.class);
        conf.setReducerClass(E_Reduce.class);

        conf.setInputFormat(org.apache.hadoop.mapred.TextInputFormat.class);
        conf.setOutputFormat(org.apache.hadoop.mapred.TextOutputFormat.class);

        FileInputFormat.setInputPaths(conf, new Path(args[0]));
        FileOutputFormat.setOutputPath(conf, new Path(args[1]));

        JobClient.runJob(conf);
    }
}








-- Phân nhóm theo các khoảng của Spending Score
SELECT 
    CASE 
        WHEN Spending_Score BETWEEN 0 AND 20 THEN '0-20'
        WHEN Spending_Score BETWEEN 21 AND 40 THEN '21-40'
        WHEN Spending_Score BETWEEN 41 AND 60 THEN '41-60'
        WHEN Spending_Score BETWEEN 61 AND 80 THEN '61-80'
        ELSE '81-100'
    END AS Spending_Score_Range,
    COUNT(*) AS count
FROM customer_data
GROUP BY Spending_Score_Range;

-- Phân nhóm theo các khoảng của Annual Income
SELECT 
    CASE 
        WHEN Annual_Income BETWEEN 0 AND 20 THEN '0-20k$'
        WHEN Annual_Income BETWEEN 21 AND 40 THEN '21-40k$'
        WHEN Annual_Income BETWEEN 41 AND 60 THEN '41-60k$'
        ELSE '60k$+'
    END AS Annual_Income_Range,
    COUNT(*) AS count
FROM customer_data
GROUP BY Annual_Income_Range;
